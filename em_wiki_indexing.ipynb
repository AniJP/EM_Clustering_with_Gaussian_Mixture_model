{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671f62a8-79b7-4984-bcbf-96ce49e3cd7e",
   "metadata": {},
   "source": [
    "Step 1: Load and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "269e0ffa-96a4-4d70-ae37-640560366fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (59071, 10000)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Load the dataset\n",
    "wiki_df = pd.read_csv(\"people_wiki.csv\")\n",
    "\n",
    "# Load index-to-word mapping\n",
    "with open(\"4_map_index_to_word.json\", \"r\") as f:\n",
    "    index_to_word = json.load(f)\n",
    "\n",
    "# Extract text data\n",
    "documents = wiki_df['text'].tolist()\n",
    "article_ids = wiki_df['URI'].tolist()\n",
    "\n",
    "# Convert documents to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Normalize the TF-IDF vectors to unit length\n",
    "tfidf_matrix = normalize(tfidf_matrix, norm='l2')\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4400c-4314-4683-9b96-e3b5398f888a",
   "metadata": {},
   "source": [
    "Step 2: Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74d3950a-705c-4fbe-8d5d-1eab4e11656a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running k-means...\n",
      "Initialization complete.\n",
      "Means shape: (5, 10000)\n",
      "Weights: [0.19212473 0.21255777 0.17201334 0.26554485 0.15775931]\n",
      "Variances shape: (5, 10000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "K = 5\n",
    "\n",
    "# Run k-means\n",
    "print(\"Running k-means...\")\n",
    "kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
    "assignments = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "means = kmeans.cluster_centers_\n",
    "\n",
    "N = tfidf_matrix.shape[0]\n",
    "weights = np.array([(assignments == k).sum() for k in range(K)]) / N\n",
    "\n",
    "variances = np.zeros((K, tfidf_matrix.shape[1]))\n",
    "\n",
    "for k in range(K):\n",
    "    cluster_points = tfidf_matrix[assignments == k]\n",
    "    if cluster_points.shape[0] > 0:\n",
    "        mean_vec = sp.csr_matrix(means[k])\n",
    "        mean_matrix = sp.vstack([mean_vec] * cluster_points.shape[0])\n",
    "        diff = cluster_points - mean_matrix\n",
    "        var = diff.multiply(diff).mean(axis=0)\n",
    "        variances[k] = np.asarray(var).flatten()\n",
    "    \n",
    "    variances[k][variances[k] < 1e-8] = 1e-8\n",
    "\n",
    "print(\"Initialization complete.\")\n",
    "print(f\"Means shape: {means.shape}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "print(f\"Variances shape: {variances.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10222eae-b495-4afe-9943-fa8c8135a8c8",
   "metadata": {},
   "source": [
    "Step 3: E-Step – Compute Responsibilities and Log-Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc5caab9-fd0e-4b81-86b0-7299fa17570b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing memory-safe E-step...\n",
      "E-step complete. Log-likelihood: 2290719663.5152\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "def compute_log_likelihood_sparse(tfidf_matrix, means, variances, weights):\n",
    "    N, D = tfidf_matrix.shape\n",
    "    K = means.shape[0]\n",
    "    log_probs = np.zeros((N, K))\n",
    "\n",
    "    for k in range(K):\n",
    "        log_det = np.sum(np.log(variances[k]))\n",
    "        const = -0.5 * (np.log(2 * np.pi) * D + log_det)\n",
    "\n",
    "        inv_var = 1.0 / variances[k]\n",
    "\n",
    "        x_sq_scaled = tfidf_matrix.multiply(tfidf_matrix).dot(inv_var)\n",
    "\n",
    "        cross_term = tfidf_matrix.dot(means[k] * inv_var)\n",
    "\n",
    "        mu_sq_term = np.sum((means[k] ** 2) * inv_var)\n",
    "\n",
    "        mahalanobis = x_sq_scaled - 2 * cross_term + mu_sq_term\n",
    "        log_probs[:, k] = const - 0.5 * mahalanobis + np.log(weights[k])\n",
    "\n",
    "    return log_probs\n",
    "\n",
    "#E-step\n",
    "print(\"Computing memory-safe E-step...\")\n",
    "log_probs = compute_log_likelihood_sparse(tfidf_matrix, means, variances, weights)\n",
    "\n",
    "log_sum = logsumexp(log_probs, axis=1)\n",
    "log_responsibilities = log_probs - log_sum[:, np.newaxis]\n",
    "responsibilities = np.exp(log_responsibilities)\n",
    "\n",
    "# Log-likelihood\n",
    "log_likelihood = np.sum(log_sum)\n",
    "print(f\"E-step complete. Log-likelihood: {log_likelihood:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003aa248-9c0a-449f-89d3-a61fb27b30ce",
   "metadata": {},
   "source": [
    "Step 4: M-Step (Maximization Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5174be26-5bb2-40d6-8b21-d17f6e27b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(tfidf_matrix, responsibilities):\n",
    "    N, D = tfidf_matrix.shape\n",
    "    K = responsibilities.shape[1]\n",
    "\n",
    "    Nk = responsibilities.sum(axis=0)  \n",
    "    \n",
    "    weights = Nk / N\n",
    "\n",
    "    means = np.zeros((K, D))\n",
    "    for k in range(K):\n",
    "        resp = responsibilities[:, k] \n",
    "        weighted_sum = tfidf_matrix.T.dot(resp) \n",
    "        means[k] = (weighted_sum / Nk[k]).A1 if sp.issparse(weighted_sum) else weighted_sum / Nk[k]\n",
    "\n",
    "    variances = np.zeros((K, D))\n",
    "    for k in range(K):\n",
    "        mu = means[k]\n",
    "        resp = responsibilities[:, k]\n",
    "        \n",
    "        x_sq = tfidf_matrix.multiply(tfidf_matrix) \n",
    "        x_sq_weighted_sum = x_sq.T.dot(resp)\n",
    "        \n",
    "        mean_sq = mu ** 2\n",
    "        ex2 = x_sq_weighted_sum / Nk[k]\n",
    "        var = ex2 - mean_sq\n",
    "        var[var < 1e-8] = 1e-8  \n",
    "        variances[k] = var\n",
    "\n",
    "    return weights, means, variances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70412b8f-67e8-4d03-ac7f-14d61c0f7605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_em(tfidf_matrix, K=5, max_iter=20, threshold=1e-4):\n",
    "    # Step 1: K-means initialization\n",
    "    kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
    "    assignments = kmeans.fit_predict(tfidf_matrix)\n",
    "    means = kmeans.cluster_centers_\n",
    "    N, D = tfidf_matrix.shape\n",
    "    weights = np.array([(assignments == k).sum() for k in range(K)]) / N\n",
    "\n",
    "    # Initial variances\n",
    "    variances = np.zeros((K, D))\n",
    "    for k in range(K):\n",
    "        cluster_points = tfidf_matrix[assignments == k]\n",
    "        if cluster_points.shape[0] > 0:\n",
    "            mean_vec = sp.csr_matrix(means[k])\n",
    "            mean_matrix = sp.vstack([mean_vec] * cluster_points.shape[0])\n",
    "            diff = cluster_points - mean_matrix\n",
    "            var = diff.multiply(diff).mean(axis=0)\n",
    "            variances[k] = np.asarray(var).flatten()\n",
    "        variances[k][variances[k] < 1e-8] = 1e-8\n",
    "\n",
    "    # Logs\n",
    "    log_likelihoods = []\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        # E-step\n",
    "        log_probs = compute_log_likelihood_sparse(tfidf_matrix, means, variances, weights)\n",
    "        log_sum = logsumexp(log_probs, axis=1)\n",
    "        log_responsibilities = log_probs - log_sum[:, np.newaxis]\n",
    "        responsibilities = np.exp(log_responsibilities)\n",
    "        log_likelihood = np.sum(log_sum)\n",
    "        log_likelihoods.append(log_likelihood)\n",
    "\n",
    "        print(f\"EM Iteration {iteration + 1}: Log-likelihood = {log_likelihood:.4f}\")\n",
    "\n",
    "        # Convergence check\n",
    "        if iteration > 0 and abs(log_likelihoods[-1] - log_likelihoods[-2]) < threshold:\n",
    "            print(\"Converged.\")\n",
    "            break\n",
    "\n",
    "        # M-step\n",
    "        weights, means, variances = m_step(tfidf_matrix, responsibilities)\n",
    "\n",
    "    return responsibilities, weights, means, variances, log_likelihoods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58e24639-5a36-42c9-965d-e3fff80f59f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM Iteration 1: Log-likelihood = 2290719663.5152\n",
      "EM Iteration 2: Log-likelihood = 2329189620.0526\n",
      "EM Iteration 3: Log-likelihood = 2340799258.5286\n",
      "EM Iteration 4: Log-likelihood = 2346019930.5890\n",
      "EM Iteration 5: Log-likelihood = 2348922207.7441\n",
      "EM Iteration 6: Log-likelihood = 2350580328.9211\n",
      "EM Iteration 7: Log-likelihood = 2351656975.2758\n",
      "EM Iteration 8: Log-likelihood = 2352472985.7280\n",
      "EM Iteration 9: Log-likelihood = 2353059926.3402\n",
      "EM Iteration 10: Log-likelihood = 2353577458.4542\n",
      "EM Iteration 11: Log-likelihood = 2353996000.4438\n",
      "EM Iteration 12: Log-likelihood = 2354236578.4606\n",
      "EM Iteration 13: Log-likelihood = 2354439863.5986\n",
      "EM Iteration 14: Log-likelihood = 2354592332.7663\n",
      "EM Iteration 15: Log-likelihood = 2354609447.9154\n",
      "EM Iteration 16: Log-likelihood = 2354619889.6355\n",
      "EM Iteration 17: Log-likelihood = 2354628244.4076\n",
      "EM Iteration 18: Log-likelihood = 2354631286.4642\n",
      "EM Iteration 19: Log-likelihood = 2354633281.2667\n",
      "EM Iteration 20: Log-likelihood = 2354633569.0225\n"
     ]
    }
   ],
   "source": [
    "responsibilities, weights, means, variances, log_likelihoods = run_em(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22716565-61c4-43be-8f08-aa4cbff43ac2",
   "metadata": {},
   "source": [
    "Code to Generate cluster_assignments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c56538ea-11c7-4064-9cc8-389ae34512d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_assignments.txt written successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cluster_ids = np.argmax(responsibilities, axis=1)\n",
    "\n",
    "with open(\"cluster_assignments.txt\", \"w\") as f:\n",
    "    for article_id, cluster_id in zip(article_ids, cluster_ids):\n",
    "        f.write(f\"{article_id}\\t{cluster_id}\\n\")\n",
    "\n",
    "print(\"cluster_assignments.txt written successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e8ab158-fa04-4e25-a26c-2cfedb9ac686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_stats.txt written successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open(\"4_map_index_to_word.json\", \"r\") as f:\n",
    "    word_to_index = json.load(f)\n",
    "\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}\n",
    "\n",
    "with open(\"cluster_stats.txt\", \"w\") as f:\n",
    "    for k in range(len(means)):\n",
    "        f.write(f\"Cluster {k}:\\n\")\n",
    "\n",
    "        top_indices = np.argsort(means[k])[::-1][:5]\n",
    "\n",
    "        for idx in top_indices:\n",
    "            word = index_to_word.get(idx, f\"[word_{idx}]\")\n",
    "            mean_val = means[k][idx]\n",
    "            var_val = variances[k][idx]\n",
    "            f.write(f\"{word}\\tmean={mean_val:.6f}\\tvariance={var_val:.6f}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"cluster_stats.txt written successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1e8cd5c-9b5b-4475-a4a6-0b947dda1280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "em_parameters.txt written successfully.\n"
     ]
    }
   ],
   "source": [
    "with open(\"em_parameters.txt\", \"w\") as f:\n",
    "    for k in range(len(means)):\n",
    "        f.write(f\"Cluster {k}:\\n\")\n",
    "        f.write(f\"Weight: {weights[k]:.6f}\\n\")\n",
    "\n",
    "        # Top 10 words in mean\n",
    "        top_indices = np.argsort(means[k])[::-1][:10]\n",
    "\n",
    "        f.write(\"Top words (mean, variance):\\n\")\n",
    "        for idx in top_indices:\n",
    "            word = index_to_word.get(idx, f\"[word_{idx}]\")\n",
    "            mean_val = means[k][idx]\n",
    "            var_val = variances[k][idx]\n",
    "            f.write(f\"{word}: mean={mean_val:.6f}, var={var_val:.6f}\\n\")\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"em_parameters.txt written successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c29ed30d-624f-4b7f-b2f7-2619f953e4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence_log.txt written successfully.\n"
     ]
    }
   ],
   "source": [
    "with open(\"convergence_log.txt\", \"w\") as f:\n",
    "    for i, ll in enumerate(log_likelihoods):\n",
    "        f.write(f\"Iteration {i + 1}: Log-likelihood = {ll:.6f}\\n\")\n",
    "\n",
    "print(\"convergence_log.txt written successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00df3eb8-0bf7-4815-af79-33d7cf781e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ascii_wordclouds.txt written successfully.\n"
     ]
    }
   ],
   "source": [
    "with open(\"ascii_wordclouds.txt\", \"w\") as f:\n",
    "    for k in range(len(means)):\n",
    "        f.write(f\"Cluster {k} ASCII Word Cloud:\\n\")\n",
    "\n",
    "        #top 15 words by mean value\n",
    "        top_indices = np.argsort(means[k])[::-1][:15]\n",
    "\n",
    "        for idx in top_indices:\n",
    "            word = index_to_word.get(idx, f\"[word_{idx}]\")\n",
    "            weight = means[k][idx]\n",
    "            count = int(weight * 1000)  # Scale word size\n",
    "            ascii_word = word * max(1, min(count, 20))  # Clip between 1 and 20\n",
    "            f.write(f\"{ascii_word}\\n\")\n",
    "\n",
    "        f.write(\"\\n\" + \"=\" * 40 + \"\\n\\n\")\n",
    "\n",
    "print(\"ascii_wordclouds.txt written successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544033a6-a86e-4b5b-9432-4633e9d7bd15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c667043c-c049-4e7c-9f08-2094fc1d2a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from output_formatter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "519ed878-c9d3-4904-a8ed-e96dc35319f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Load article data\n",
    "wiki_df = pd.read_csv(\"people_wiki.csv\")\n",
    "documents = wiki_df['text'].tolist()\n",
    "article_ids = wiki_df['URI'].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "tfidf_matrix = normalize(tfidf_matrix, norm='l2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93beff5f-c6dd-4d89-861a-ee2cac8a5758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM Iteration 1: Log-likelihood = 2290719663.5152\n",
      "EM Iteration 2: Log-likelihood = 2329189620.0526\n",
      "EM Iteration 3: Log-likelihood = 2340799258.5286\n",
      "EM Iteration 4: Log-likelihood = 2346019930.5890\n",
      "EM Iteration 5: Log-likelihood = 2348922207.7441\n",
      "EM Iteration 6: Log-likelihood = 2350580328.9211\n",
      "EM Iteration 7: Log-likelihood = 2351656975.2758\n",
      "EM Iteration 8: Log-likelihood = 2352472985.7280\n",
      "EM Iteration 9: Log-likelihood = 2353059926.3402\n",
      "EM Iteration 10: Log-likelihood = 2353577458.4542\n",
      "EM Iteration 11: Log-likelihood = 2353996000.4438\n",
      "EM Iteration 12: Log-likelihood = 2354236578.4606\n",
      "EM Iteration 13: Log-likelihood = 2354439863.5986\n",
      "EM Iteration 14: Log-likelihood = 2354592332.7663\n",
      "EM Iteration 15: Log-likelihood = 2354609447.9154\n",
      "EM Iteration 16: Log-likelihood = 2354619889.6355\n",
      "EM Iteration 17: Log-likelihood = 2354628244.4076\n",
      "EM Iteration 18: Log-likelihood = 2354631286.4642\n",
      "EM Iteration 19: Log-likelihood = 2354633281.2667\n",
      "EM Iteration 20: Log-likelihood = 2354633569.0225\n"
     ]
    }
   ],
   "source": [
    "responsibilities, weights, means, variances, log_likelihoods = run_em(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6b5a1cd-1650-4de1-9cb5-d597e61a92c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = load_index_to_word(\"4_map_index_to_word.json\")\n",
    "\n",
    "save_cluster_assignments(article_ids, responsibilities)\n",
    "save_cluster_stats(means, variances, index_to_word)\n",
    "save_em_parameters(weights, means, variances, index_to_word)\n",
    "save_convergence_log(log_likelihoods)\n",
    "save_ascii_wordclouds(means, index_to_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47ce4b5f-f7c3-487e-a6ed-dcd3c8bfff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ascii_wordclouds(means, index_to_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d5a809da-5ce1-4d56-8577-6c224a458ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/anirudhjp/Downloads/people_wiki.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "899047c4-93a7-4eb4-8152-a74a122f774f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URI</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Digby_Morrell&gt;</td>\n",
       "      <td>Digby Morrell</td>\n",
       "      <td>digby morrell born 10 october 1979 is a former...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Alfred_J._Lewy&gt;</td>\n",
       "      <td>Alfred J. Lewy</td>\n",
       "      <td>alfred j lewy aka sandy lewy graduated from un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Harpdog_Brown&gt;</td>\n",
       "      <td>Harpdog Brown</td>\n",
       "      <td>harpdog brown is a singer and harmonica player...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Franz_Rottensteiner&gt;</td>\n",
       "      <td>Franz Rottensteiner</td>\n",
       "      <td>franz rottensteiner born in waidmannsfeld lowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/G-Enka&gt;</td>\n",
       "      <td>G-Enka</td>\n",
       "      <td>henry krvits born 30 december 1974 in tallinn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59066</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Olari_Elts&gt;</td>\n",
       "      <td>Olari Elts</td>\n",
       "      <td>olari elts born april 27 1971 in tallinn eston...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59067</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Scott_F._Crago&gt;</td>\n",
       "      <td>Scott F. Crago</td>\n",
       "      <td>scott francis crago born july 26 1963 twin bro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59068</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/David_Cass_(footb...</td>\n",
       "      <td>David Cass (footballer)</td>\n",
       "      <td>david william royce cass born 27 march 1962 in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59069</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Keith_Elias&gt;</td>\n",
       "      <td>Keith Elias</td>\n",
       "      <td>keith hector elias born february 3 1972 in lac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59070</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Fawaz_Damrah&gt;</td>\n",
       "      <td>Fawaz Damrah</td>\n",
       "      <td>fawaz mohammed damrah arabic fawwz damra was t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59071 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     URI  \\\n",
       "0            <http://dbpedia.org/resource/Digby_Morrell>   \n",
       "1           <http://dbpedia.org/resource/Alfred_J._Lewy>   \n",
       "2            <http://dbpedia.org/resource/Harpdog_Brown>   \n",
       "3      <http://dbpedia.org/resource/Franz_Rottensteiner>   \n",
       "4                   <http://dbpedia.org/resource/G-Enka>   \n",
       "...                                                  ...   \n",
       "59066           <http://dbpedia.org/resource/Olari_Elts>   \n",
       "59067       <http://dbpedia.org/resource/Scott_F._Crago>   \n",
       "59068  <http://dbpedia.org/resource/David_Cass_(footb...   \n",
       "59069          <http://dbpedia.org/resource/Keith_Elias>   \n",
       "59070         <http://dbpedia.org/resource/Fawaz_Damrah>   \n",
       "\n",
       "                          name  \\\n",
       "0                Digby Morrell   \n",
       "1               Alfred J. Lewy   \n",
       "2                Harpdog Brown   \n",
       "3          Franz Rottensteiner   \n",
       "4                       G-Enka   \n",
       "...                        ...   \n",
       "59066               Olari Elts   \n",
       "59067           Scott F. Crago   \n",
       "59068  David Cass (footballer)   \n",
       "59069              Keith Elias   \n",
       "59070             Fawaz Damrah   \n",
       "\n",
       "                                                    text  \n",
       "0      digby morrell born 10 october 1979 is a former...  \n",
       "1      alfred j lewy aka sandy lewy graduated from un...  \n",
       "2      harpdog brown is a singer and harmonica player...  \n",
       "3      franz rottensteiner born in waidmannsfeld lowe...  \n",
       "4      henry krvits born 30 december 1974 in tallinn ...  \n",
       "...                                                  ...  \n",
       "59066  olari elts born april 27 1971 in tallinn eston...  \n",
       "59067  scott francis crago born july 26 1963 twin bro...  \n",
       "59068  david william royce cass born 27 march 1962 in...  \n",
       "59069  keith hector elias born february 3 1972 in lac...  \n",
       "59070  fawaz mohammed damrah arabic fawwz damra was t...  \n",
       "\n",
       "[59071 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c694c7-b75b-4cd8-b318-d9331ccd89bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the full dataset\n",
    "df = pd.read_csv(\"/Users/anirudhjp/Downloads/people_wiki.csv\")\n",
    "\n",
    "# Take a small sample, e.g. 500 articles\n",
    "df_small = df.sample(n=150, random_state=42)\n",
    "\n",
    "# Save to new CSV\n",
    "df_small.to_csv(\"people_wiki_subset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ec5e62-db85-448b-ac35-d441be6a35a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (150, 9160)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Load the dataset\n",
    "wiki_df = pd.read_csv(\"people_wiki_subset.csv\")\n",
    "\n",
    "# Load index-to-word mapping\n",
    "with open(\"4_map_index_to_word.json\", \"r\") as f:\n",
    "    index_to_word = json.load(f)\n",
    "\n",
    "# Extract text data\n",
    "documents = wiki_df['text'].tolist()\n",
    "article_ids = wiki_df['URI'].tolist()\n",
    "\n",
    "# Convert documents to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Normalize the TF-IDF vectors to unit length\n",
    "tfidf_matrix = normalize(tfidf_matrix, norm='l2')\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce382c3a-2dad-4669-a17a-0400f361c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"people_wiki.csv\")\n",
    "df_subset = df.sample(n=150, random_state=42)\n",
    "df_subset.to_csv(\"people_wiki_subset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50b6aaef-5194-420b-bfaf-3699e3399c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"people_wiki_subset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6857398e-2b23-4755-82c2-d1498d25a290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM Iteration 1: Log-likelihood = 9040701.4236\n",
      "EM Iteration 2: Log-likelihood = 9040701.4236\n",
      "Converged.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from em_algorithm import run_em\n",
    "\n",
    "# Load the small subset\n",
    "df = pd.read_csv(\"people_wiki_subset.csv\")\n",
    "documents = df[\"text\"].tolist()\n",
    "article_ids = df[\"URI\"].tolist()\n",
    "\n",
    "# TF-IDF processing\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "tfidf_matrix = normalize(tfidf_matrix, norm='l2')\n",
    "\n",
    "# Run EM\n",
    "responsibilities, weights, means, variances, log_likelihoods = run_em(tfidf_matrix, K=5, max_iter=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79ac6af3-28e8-44ad-bb66-86a55f487d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASCII word clouds saved to sample_outputs/ascii_wordclouds.txt\n"
     ]
    }
   ],
   "source": [
    "from output_formatter import *\n",
    "from visualizer import save_ascii_wordclouds\n",
    "\n",
    "index_to_word = load_index_to_word(\"4_map_index_to_word.json\")\n",
    "\n",
    "import os\n",
    "os.makedirs(\"sample_outputs\", exist_ok=True)\n",
    "\n",
    "save_cluster_assignments(article_ids, responsibilities, filename=\"sample_outputs/cluster_assignments.txt\")\n",
    "save_cluster_stats(means, variances, index_to_word, filename=\"sample_outputs/cluster_stats.txt\")\n",
    "save_em_parameters(weights, means, variances, index_to_word, filename=\"sample_outputs/em_parameters.txt\")\n",
    "save_convergence_log(log_likelihoods, filename=\"sample_outputs/convergence_log.txt\")\n",
    "save_ascii_wordclouds(means, index_to_word, filename=\"sample_outputs/ascii_wordclouds.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db72ad29-a758-497a-bf40-34c21adb471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 ASCII Word Cloud:\n",
      "1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton\n",
      "vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988\n",
      "vivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivacious\n",
      "etcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalf\n",
      "biologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayor\n",
      "kilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynolds\n",
      "yasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasuto\n",
      "opheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaophelia\n",
      "diamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonn\n",
      "essayistryanessayistryanessayistryanessayistryanessayistryanessayistryanessayistryanessayistryanessayistryanessayistryanessayistryanessayistryanessayistryanessayistryanessayistryanessayistryanessayistryanessayistryanessayistryanessayistryan\n",
      "angilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilley\n",
      "battlegroupbattlegroupbattlegroupbattlegroupbattlegroupbattlegroupbattlegroupbattlegroupbattlegroupbattlegroupbattlegroupbattlegroupbattlegroupbattlegroupbattlegroupbattlegroupbattlegroupbattlegroupbattlegroupbattlegroup\n",
      "politiciandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandaba\n",
      "26202620262026202620262026202620262026202620262026202620262026202620262026202620\n",
      "easternmosteasternmosteasternmosteasternmosteasternmosteasternmosteasternmosteasternmosteasternmosteasternmosteasternmosteasternmosteasternmosteasternmosteasternmosteasternmosteasternmosteasternmosteasternmosteasternmost\n",
      "\n",
      "========================================\n",
      "\n",
      "Cluster 1 ASCII Word Cloud:\n",
      "1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton\n",
      "tractionnellenstractionnellenstractionnellenstractionnellenstractionnellenstractionnellenstractionnellenstractionnellenstractionnellenstractionnellenstractionnellenstractionnellenstractionnellenstractionnellenstractionnellenstractionnellenstractionnellenstractionnellenstractionnellenstractionnellens\n",
      "dreamer2dreamer2dreamer2dreamer2dreamer2dreamer2dreamer2dreamer2dreamer2dreamer2dreamer2dreamer2dreamer2dreamer2dreamer2dreamer2dreamer2dreamer2dreamer2dreamer2\n",
      "vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988\n",
      "biologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayor\n",
      "vivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivacious\n",
      "kilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynolds\n",
      "diamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonn\n",
      "opheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaophelia\n",
      "angilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilley\n",
      "singalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongs\n",
      "politiciandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandaba\n",
      "citizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenonce\n",
      "pazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzi\n",
      "shakespeareinshakespeareinshakespeareinshakespeareinshakespeareinshakespeareinshakespeareinshakespeareinshakespeareinshakespeareinshakespeareinshakespeareinshakespeareinshakespeareinshakespeareinshakespeareinshakespeareinshakespeareinshakespeareinshakespearein\n",
      "\n",
      "========================================\n",
      "\n",
      "Cluster 2 ASCII Word Cloud:\n",
      "1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton\n",
      "vivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivacious\n",
      "biologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayor\n",
      "vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988\n",
      "etcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalf\n",
      "kingstonuponthameskingstonuponthameskingstonuponthameskingstonuponthameskingstonuponthameskingstonuponthameskingstonuponthameskingstonuponthameskingstonuponthameskingstonuponthameskingstonuponthameskingstonuponthameskingstonuponthameskingstonuponthameskingstonuponthameskingstonuponthameskingstonuponthameskingstonuponthameskingstonuponthameskingstonuponthames\n",
      "germplasmgermplasmgermplasmgermplasmgermplasmgermplasmgermplasmgermplasmgermplasmgermplasmgermplasmgermplasmgermplasmgermplasmgermplasmgermplasmgermplasmgermplasmgermplasmgermplasm\n",
      "opheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaophelia\n",
      "angilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilleyangilley\n",
      "salafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafist\n",
      "politiciandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandaba\n",
      "kilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynolds\n",
      "seatbucklaschukseatbucklaschukseatbucklaschukseatbucklaschukseatbucklaschukseatbucklaschukseatbucklaschukseatbucklaschukseatbucklaschukseatbucklaschukseatbucklaschukseatbucklaschukseatbucklaschukseatbucklaschukseatbucklaschukseatbucklaschukseatbucklaschukseatbucklaschukseatbucklaschukseatbucklaschuk\n",
      "yasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasuto\n",
      "pricedpricedpricedpricedpricedpricedpricedpricedpricedpricedpricedpricedpricedpricedpricedpricedpricedpricedpricedpriced\n",
      "\n",
      "========================================\n",
      "\n",
      "Cluster 3 ASCII Word Cloud:\n",
      "1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton\n",
      "vivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivacious\n",
      "vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988\n",
      "biologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayor\n",
      "etcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalf\n",
      "opheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaophelia\n",
      "kilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynolds\n",
      "yasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasuto\n",
      "citizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenonce\n",
      "singalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongs\n",
      "funinfuninfuninfuninfuninfuninfuninfuninfuninfuninfuninfuninfuninfuninfuninfuninfuninfuninfuninfunin\n",
      "salafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafistsalafist\n",
      "politiciandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandaba\n",
      "kwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrentlykwaniewskicurrently\n",
      "pazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzipazzi\n",
      "\n",
      "========================================\n",
      "\n",
      "Cluster 4 ASCII Word Cloud:\n",
      "1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton1980slangton\n",
      "biologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayorbiologymayor\n",
      "vivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivaciousvivacious\n",
      "vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988vikings1988\n",
      "etcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalfetcmetcalf\n",
      "diamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonndiamondzonn\n",
      "kilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynoldskilgorereynolds\n",
      "singalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongssingalongs\n",
      "hunhunhunhunhunhunhunhunhunhunhunhunhunhunhunhunhunhunhunhun\n",
      "politiciandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandabapoliticiandaba\n",
      "citizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenoncecitizenonce\n",
      "opheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaopheliaophelia\n",
      "2014fila2014fila2014fila2014fila2014fila2014fila2014fila2014fila2014fila2014fila2014fila2014fila2014fila2014fila2014fila2014fila2014fila2014fila2014fila2014fila\n",
      "yasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasutoyasuto\n",
      "2010borges2010borges2010borges2010borges2010borges2010borges2010borges2010borges2010borges2010borges2010borges2010borges2010borges2010borges2010borges2010borges2010borges2010borges2010borges2010borges\n",
      "\n",
      "========================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"sample_outputs/ascii_wordclouds.txt\", \"r\") as f:\n",
    "    content = f.read()\n",
    "    print(content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
